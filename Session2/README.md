# Neural Net Basics

Neural network neuron is a storage unit that stores a number or a "signal". Our neurons in our brain in addition to having a storage unit, also have their own computation unit. A neural network neuron has the computation unit outside it along with a weight (a number) for each neuron. A NN neuron stores the result of the computation of its input based on the weight and the activation function

### Let us see forward and backward pass of a small Neural network in MS Excel.

![Network Structure](https://user-images.githubusercontent.com/39134120/118151499-535e0580-b431-11eb-9248-c29fef488986.JPG)


The above image shows the architecture of the Neural network.<br>
We have considered a samller network with less layers as we are doing the entire excercise in MS Excel.<br>
i1 and i2 are the input layer neurons.<br>
h1 and h2 are the hidden layer neurons.<br>
a_h1 and a_h2 are activated outputs from hidden layer neuron h1 and h2.<br>
o1 and o2 are the neuron connected to the output layer.<br>
a_o1 and a_o2 are activated output of o1 and o2 neuron.<br>
E_total is the total error of the network.<br>
w1 and w3 are the weights from input layer neuron i1 to h1 and h2 respectively.<br>
w2 and w4 are the weights from input layer neuron i2 to h1 and h2 respectively.<br>
w5 and w7 are the weights from activated hidden layer neuron a_h1 to o1 and o2 respectively.<br>
w6 and w8 are the weights from activated hidden layer neuron a_h2 to o1 and o2 respectively.<br>

Follwing are the values calculated for different parameters of the above shown Neural network.

![NN parameters](https://user-images.githubusercontent.com/39134120/126879579-725ee132-39f2-4fd2-ba0c-db2dff80dba9.JPG)


We know that the chain rule is used for calculating the derivative of composite functions. If a variable z depends on the variable y, which itself depends on the variable x, so that y and z are dependent variables, then z, via the intermediate variable of y, depends on x as well. This is called the chain rule.Just like this the input/hidden layer neuron has impact on the final output obtained in the final layer.The weights associated with the input/hidden layer is optimized via their association with the output layer which itself updated via loss calculated from difference between predicted and actual value.



<img width="745" alt="backprop-1" src="https://user-images.githubusercontent.com/39134120/126879428-83ffae65-f3f7-4ab7-ae9b-0644ee2f30a3.png">


So using above concept when we calculated values for different parameters we got the following -

![Weight_calc1](https://user-images.githubusercontent.com/39134120/118123847-70371080-b412-11eb-845d-3eb86bb9555d.JPG)
![Weight_calc2](https://user-images.githubusercontent.com/39134120/118123881-79c07880-b412-11eb-8126-b233d1d70596.JPG)
![Weight_calc3](https://user-images.githubusercontent.com/39134120/118123906-804ef000-b412-11eb-91e1-61adcdb872ed.JPG)
![Weight_calc4](https://user-images.githubusercontent.com/39134120/118123929-8644d100-b412-11eb-9847-e6c8b4b9a8db.JPG)

We used learning rate of 0.5<br>
![learning rate](https://user-images.githubusercontent.com/39134120/118124146-cdcb5d00-b412-11eb-9d30-f60148954141.JPG)

After using the formula obtained by applying chain rule for backward pass and running for 39 epochs (dragged the formula to 39 observations) we got the following.
![final_weights](https://user-images.githubusercontent.com/39134120/118124503-4df1c280-b413-11eb-9f91-e50b06e121a5.JPG)

when we plot the error rate with respect to the number of epochs we got the following graph.
![Trained network_Error_vs_Epochs](https://user-images.githubusercontent.com/39134120/118124626-7a0d4380-b413-11eb-9731-d38dfdc88444.JPG)

We tried with different learning rate and following are the graphs for different learning rates.

Learning rate =0.1<br>
![learning_rate0 1](https://user-images.githubusercontent.com/39134120/118125674-ec325800-b414-11eb-88aa-820b2f328e23.JPG)

The network is not converging fast enough due to low learning rate.

Learning rate =0.2 <br>
![learning_rate0 2](https://user-images.githubusercontent.com/39134120/118125803-12f08e80-b415-11eb-9319-b796cab89ccc.JPG)

Better convergence compared to learning rate of 0.1 but still network not converging fast enough due to low learning rate.

Learning rate =0.5<br>
![learning_rate0 5](https://user-images.githubusercontent.com/39134120/118127381-3e747880-b417-11eb-947a-534b35a86fb4.JPG)

Better convergence of network at learning rate of 0.5 compared to 0.1 and 0.2.

Learning rate =0.8<br>

![learning_rate0 8](https://user-images.githubusercontent.com/39134120/118126206-a5912d80-b415-11eb-9fba-14b0c5295f05.JPG)

Better convergence of network at learning rate of 0.8 compared to 0.1,0.2 and 0.5.

Learning rate =1<br>
![learning_rate1](https://user-images.githubusercontent.com/39134120/118126555-1fc1b200-b416-11eb-98ba-0dc28a0494ce.JPG)

Better convergence of network compared to learning rate of 0.1,0.2,0.5 and 0.8. Since this is a small network learning rate of 1 is giving 
good convergence but it is not advisable to start with learning rate of 1 for neural network with more number of layers.

Learning rate =2<br>

![learning_rate2](https://user-images.githubusercontent.com/39134120/118126820-7cbd6800-b416-11eb-8c6a-d388f6f8d94a.JPG)

Better convergence of network compared to all previous learning rates. Since this is a small network learning rate of 2 is giving 
good convergence but it is not advisable to start with learning rate of 2 for neural network with more number of layers.



Note:- All details of weight calculation can be found in "Neural_Net_excel.xlsx" file placed on this repository.







